{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "a9e598214dce2f7aff9389c58f6871a8506966d8"
   },
   "outputs": [],
   "source": [
    "data =  pd.read_csv('data/train.csv', nrows = 15_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Given a dataframe, add two new features 'abs_diff_longitude' and\n",
    "# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n",
    "# the pickup location to the dropoff location.\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "add_travel_vector_features(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "3501b1a8a57342293c6ab6f3e5779858b9cf60ff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old size: 15000\n",
      "New size: 15000\n"
     ]
    }
   ],
   "source": [
    "print('Old size: %d' % len(data))\n",
    "data = data.dropna(how = 'any', axis = 'rows')\n",
    "print('New size: %d' % len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "c51a15f4a365862f62dea455f4c2462d2190ba4e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "data['datetime_object'] = [datetime.strptime(date,'%Y-%m-%d %H:%M:%S %Z') for date in data['pickup_datetime']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "4fc9f4307938fa4acf3a91b5a54363c14357d501",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>abs_diff_longitude</th>\n",
       "      <th>abs_diff_latitude</th>\n",
       "      <th>datetime_object</th>\n",
       "      <th>distance_miles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-06-15 17:26:21.0000001</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2009-06-15 17:26:21 UTC</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>2009-06-15 17:26:21</td>\n",
       "      <td>0.640487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05 16:52:16.0000002</td>\n",
       "      <td>16.9</td>\n",
       "      <td>2010-01-05 16:52:16 UTC</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036780</td>\n",
       "      <td>0.070701</td>\n",
       "      <td>2010-01-05 16:52:16</td>\n",
       "      <td>5.250670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-08-18 00:35:00.00000049</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2011-08-18 00:35:00 UTC</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>2</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>2011-08-18 00:35:00</td>\n",
       "      <td>0.863411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-04-21 04:30:42.0000001</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2012-04-21 04:30:42 UTC</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.024949</td>\n",
       "      <td>2012-04-21 04:30:42</td>\n",
       "      <td>1.739386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-03-09 07:51:00.000000135</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2010-03-09 07:51:00 UTC</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>0.015754</td>\n",
       "      <td>2010-03-09 07:51:00</td>\n",
       "      <td>1.242218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             key  fare_amount          pickup_datetime  \\\n",
       "0    2009-06-15 17:26:21.0000001          4.5  2009-06-15 17:26:21 UTC   \n",
       "1    2010-01-05 16:52:16.0000002         16.9  2010-01-05 16:52:16 UTC   \n",
       "2   2011-08-18 00:35:00.00000049          5.7  2011-08-18 00:35:00 UTC   \n",
       "3    2012-04-21 04:30:42.0000001          7.7  2012-04-21 04:30:42 UTC   \n",
       "4  2010-03-09 07:51:00.000000135          5.3  2010-03-09 07:51:00 UTC   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "0        -73.844311        40.721319         -73.841610         40.712278   \n",
       "1        -74.016048        40.711303         -73.979268         40.782004   \n",
       "2        -73.982738        40.761270         -73.991242         40.750562   \n",
       "3        -73.987130        40.733143         -73.991567         40.758092   \n",
       "4        -73.968095        40.768008         -73.956655         40.783762   \n",
       "\n",
       "   passenger_count  abs_diff_longitude  abs_diff_latitude     datetime_object  \\\n",
       "0                1            0.002701           0.009041 2009-06-15 17:26:21   \n",
       "1                1            0.036780           0.070701 2010-01-05 16:52:16   \n",
       "2                2            0.008504           0.010708 2011-08-18 00:35:00   \n",
       "3                1            0.004437           0.024949 2012-04-21 04:30:42   \n",
       "4                1            0.011440           0.015754 2010-03-09 07:51:00   \n",
       "\n",
       "   distance_miles  \n",
       "0        0.640487  \n",
       "1        5.250670  \n",
       "2        0.863411  \n",
       "3        1.739386  \n",
       "4        1.242218  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(data.describe())\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = 0.017453292519943295 # Pi/180\n",
    "    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n",
    "    return 0.6213712 * 12742 * np.arcsin(np.sqrt(a)) # 2*R*asin...\n",
    "\n",
    "# add new column to dataframe with distance in miles\n",
    "data['distance_miles'] = distance(data.pickup_latitude, data.pickup_longitude, \\\n",
    "                                      data.dropoff_latitude, data.dropoff_longitude)\n",
    "\n",
    "(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "1dc7077a9940f07baf0cdcda99f2c01d991894b9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old size: 15000\n",
      "New size: 14388\n"
     ]
    }
   ],
   "source": [
    "print('Old size: %d' % len(data))\n",
    "data = data[(data.abs_diff_longitude < 3.0) & (data.abs_diff_latitude < 3.0)]\n",
    "data = data[(data.pickup_longitude >= -74.3) & (data.pickup_longitude <= -72.9)]  # nyc coordinates\n",
    "data = data[(data.dropoff_longitude >= -74.3) & (data.dropoff_longitude <= -72.9)]\n",
    "data = data[(data.pickup_latitude >= 40.5) & (data.pickup_latitude <= 41.8)]\n",
    "data = data[(data.dropoff_latitude >= 40.5) & (data.dropoff_latitude <= 41.8)]\n",
    "data = data[(data.fare_amount>=2) & (data.fare_amount<=500)]\n",
    "data = data[(data.passenger_count>0) & (data.passenger_count <=6)]\n",
    "data = data[(data.distance_miles<=100.0) & (data.distance_miles>0.05)]\n",
    "nyc = (-74.0063889, 40.7141667)\n",
    "data['distance_to_center'] = distance(nyc[1], nyc[0],data.dropoff_latitude, data.dropoff_longitude)\n",
    "data = data[data.distance_to_center<15.0]\n",
    "print('New size: %d' % len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "0587baf20567e6855e9a8342646eb3cc607b6cda"
   },
   "outputs": [],
   "source": [
    "def late_night (row):\n",
    "    if (row['hour'] <= 6) or (row['hour'] >= 20):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def night (row):\n",
    "    if ((row['hour'] <= 20) and (row['hour'] >= 16)) and (row['weekday'] < 5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "\n",
    "#data.distance_miles.hist(bins=50, figsize=(12,4))\n",
    "#plt.xlabel('distance miles')\n",
    "#plt.title('Histogram ride distances in miles')\n",
    "#data.groupby('passenger_count')['distance_miles', 'fare_amount'].mean()\n",
    "#print(\"Average $USD/Mile : {:0.2f}\".format(data.fare_amount.sum()/data.distance_miles.sum()))\n",
    "#data['fare_per_mile'] = data.fare_amount / data.distance_miles\n",
    "data['hour'] = [date.hour for date in data['datetime_object']]\n",
    "data['year'] = [date.year for date in data['datetime_object']]\n",
    "data['day'] = [date.day for date in data['datetime_object']]\n",
    "data['weekday'] = data['datetime_object'].apply(lambda x: x.weekday())\n",
    "data['night'] = data.apply (lambda x: night(x), axis=1)\n",
    "data['late_night'] = data.apply (lambda x: late_night(x), axis=1)   \n",
    "# There is a $1 surcharge from 4pm to 8pm on weekdays, excluding holidays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c647154ffae4423a6626432c846c8fed188e9cb5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ardroid\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ardroid\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5886: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\ardroid\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ardroid\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ardroid\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ardroid\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5886: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\ardroid\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ardroid\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ardroid\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5886: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\ardroid\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>abs_diff_longitude</th>\n",
       "      <th>abs_diff_latitude</th>\n",
       "      <th>distance_miles</th>\n",
       "      <th>distance_to_center</th>\n",
       "      <th>...</th>\n",
       "      <th>Nassau</th>\n",
       "      <th>Suffolk</th>\n",
       "      <th>Westchester</th>\n",
       "      <th>Rockland</th>\n",
       "      <th>Dutchess</th>\n",
       "      <th>Orange</th>\n",
       "      <th>Putnam</th>\n",
       "      <th>airport</th>\n",
       "      <th>county1</th>\n",
       "      <th>county2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1170.00000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.548162</td>\n",
       "      <td>-73.906927</td>\n",
       "      <td>40.739297</td>\n",
       "      <td>-73.913343</td>\n",
       "      <td>40.741902</td>\n",
       "      <td>1.690598</td>\n",
       "      <td>0.117626</td>\n",
       "      <td>0.054102</td>\n",
       "      <td>7.584955</td>\n",
       "      <td>6.772515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91453</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.068376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.004274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.067739</td>\n",
       "      <td>0.093403</td>\n",
       "      <td>0.063502</td>\n",
       "      <td>0.076487</td>\n",
       "      <td>0.055325</td>\n",
       "      <td>1.288893</td>\n",
       "      <td>0.073917</td>\n",
       "      <td>0.056775</td>\n",
       "      <td>4.965965</td>\n",
       "      <td>3.491510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27970</td>\n",
       "      <td>0.071458</td>\n",
       "      <td>0.252498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453440</td>\n",
       "      <td>0.150212</td>\n",
       "      <td>0.065260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>-74.017173</td>\n",
       "      <td>40.604462</td>\n",
       "      <td>-74.183955</td>\n",
       "      <td>40.574652</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.052675</td>\n",
       "      <td>0.032793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.500000</td>\n",
       "      <td>-73.981932</td>\n",
       "      <td>40.721321</td>\n",
       "      <td>-73.975102</td>\n",
       "      <td>40.711688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076255</td>\n",
       "      <td>0.014541</td>\n",
       "      <td>5.090898</td>\n",
       "      <td>3.711352</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>31.390000</td>\n",
       "      <td>-73.922901</td>\n",
       "      <td>40.755746</td>\n",
       "      <td>-73.916302</td>\n",
       "      <td>40.753014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111171</td>\n",
       "      <td>0.035972</td>\n",
       "      <td>6.446991</td>\n",
       "      <td>6.741268</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42.790000</td>\n",
       "      <td>-73.863327</td>\n",
       "      <td>40.771234</td>\n",
       "      <td>-73.865549</td>\n",
       "      <td>40.770752</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.153551</td>\n",
       "      <td>0.088750</td>\n",
       "      <td>10.085346</td>\n",
       "      <td>8.875205</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>-73.137393</td>\n",
       "      <td>41.366138</td>\n",
       "      <td>-73.726348</td>\n",
       "      <td>40.911461</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.863547</td>\n",
       "      <td>0.634415</td>\n",
       "      <td>62.817285</td>\n",
       "      <td>14.801348</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "count  1170.000000       1170.000000      1170.000000        1170.000000   \n",
       "mean     32.548162        -73.906927        40.739297         -73.913343   \n",
       "std      15.067739          0.093403         0.063502           0.076487   \n",
       "min       2.500000        -74.017173        40.604462         -74.183955   \n",
       "25%      22.500000        -73.981932        40.721321         -73.975102   \n",
       "50%      31.390000        -73.922901        40.755746         -73.916302   \n",
       "75%      42.790000        -73.863327        40.771234         -73.865549   \n",
       "max      88.000000        -73.137393        41.366138         -73.726348   \n",
       "\n",
       "       dropoff_latitude  passenger_count  abs_diff_longitude  \\\n",
       "count       1170.000000      1170.000000         1170.000000   \n",
       "mean          40.741902         1.690598            0.117626   \n",
       "std            0.055325         1.288893            0.073917   \n",
       "min           40.574652         1.000000            0.000307   \n",
       "25%           40.711688         1.000000            0.076255   \n",
       "50%           40.753014         1.000000            0.111171   \n",
       "75%           40.770752         2.000000            0.153551   \n",
       "max           40.911461         6.000000            0.863547   \n",
       "\n",
       "       abs_diff_latitude  distance_miles  distance_to_center     ...       \\\n",
       "count        1170.000000     1170.000000         1170.000000     ...        \n",
       "mean            0.054102        7.584955            6.772515     ...        \n",
       "std             0.056775        4.965965            3.491510     ...        \n",
       "min             0.000085        0.052675            0.032793     ...        \n",
       "25%             0.014541        5.090898            3.711352     ...        \n",
       "50%             0.035972        6.446991            6.741268     ...        \n",
       "75%             0.088750       10.085346            8.875205     ...        \n",
       "max             0.634415       62.817285           14.801348     ...        \n",
       "\n",
       "           Nassau      Suffolk  Westchester  Rockland  Dutchess  Orange  \\\n",
       "count  1170.00000  1170.000000  1170.000000    1170.0    1170.0  1170.0   \n",
       "mean      0.91453     0.005128     0.068376       0.0       0.0     0.0   \n",
       "std       0.27970     0.071458     0.252498       0.0       0.0     0.0   \n",
       "min       0.00000     0.000000     0.000000       0.0       0.0     0.0   \n",
       "25%       1.00000     0.000000     0.000000       0.0       0.0     0.0   \n",
       "50%       1.00000     0.000000     0.000000       0.0       0.0     0.0   \n",
       "75%       1.00000     0.000000     0.000000       0.0       0.0     0.0   \n",
       "max       1.00000     1.000000     1.000000       0.0       0.0     0.0   \n",
       "\n",
       "       Putnam      airport      county1      county2  \n",
       "count  1170.0  1170.000000  1170.000000  1170.000000  \n",
       "mean      0.0     0.711111     0.976923     0.004274  \n",
       "std       0.0     0.453440     0.150212     0.065260  \n",
       "min       0.0     0.000000     0.000000     0.000000  \n",
       "25%       0.0     0.000000     1.000000     0.000000  \n",
       "50%       0.0     1.000000     1.000000     0.000000  \n",
       "75%       0.0     1.000000     1.000000     0.000000  \n",
       "max       0.0     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rangeA = 1.5\n",
    "rangeN = 20.0\n",
    "rangeS = 48.7\n",
    "rangeR = 14.1\n",
    "rangeD = 28.7\n",
    "rangeO = 29\n",
    "rangeP = 15.7\n",
    "rangeW = 20.0\n",
    "jfk = (-73.7822222222, 40.6441666667) #JFK Airport\n",
    "ewr = (-74.175, 40.69) # Newark Liberty International Airport\n",
    "lgr = (-73.87, 40.77) # LaGuardia Airport\n",
    "\n",
    " # county\n",
    "Nassau = (-73.5594, 40.6546)\n",
    "Suffolk = (-72.6151, 40.9849)\n",
    "Westchester = (-73.7949, 41.1220)\n",
    "Rockland = (-73.9830, 41.1489)\n",
    "Dutchess = (-73.7478, 41.7784)\n",
    "Orange = (-74.3118, 41.3912)\n",
    "Putnam = (-73.7949, 41.4351) \n",
    "\n",
    "data_air=data\n",
    "\n",
    "def add_checkpoint(point, point_name,rangeA):\n",
    "    data_air[point_name] = (distance(data.pickup_latitude, data.pickup_longitude, point[1], point[0]) <= rangeA) | ((distance(data.dropoff_latitude, data.dropoff_longitude, point[1], point[0]) <= rangeA))\n",
    "    data_air[point_name].replace(False, 0, inplace=True)\n",
    "    data_air[point_name] = data_air[point_name].astype(int)\n",
    "\n",
    "add_checkpoint(jfk, 'jfk',rangeA)\n",
    "add_checkpoint(ewr, 'ewr',rangeA)\n",
    "add_checkpoint(lgr, 'lgr',rangeA)\n",
    "add_checkpoint(Nassau, 'Nassau',rangeN)\n",
    "add_checkpoint(Suffolk, 'Suffolk',rangeS)\n",
    "add_checkpoint(Westchester, 'Westchester',rangeW)\n",
    "add_checkpoint(Rockland, 'Rockland',rangeR )\n",
    "add_checkpoint(Dutchess, 'Dutchess',rangeD)\n",
    "add_checkpoint(Orange, 'Orange',rangeO)\n",
    "add_checkpoint(Putnam, 'Putnam',rangeP)\n",
    "\n",
    "data_air = data[(data_air.jfk | data_air.ewr | data_air.lgr | data_air.Nassau | data_air.Suffolk | data_air.Westchester | data_air.Rockland | data_air.Dutchess | data_air.Orange | data_air.Putnam)==1]\n",
    "data_air['airport'] = (data_air.jfk | data_air.ewr | data_air.lgr )==1\n",
    "data_air['airport'].replace(False, 0, inplace=True)\n",
    "data_air['airport'] = data_air['airport'].astype(int)\n",
    "data_air['county1'] = (data_air.jfk | data_air.ewr | data_air.lgr )==0\n",
    "data_air['county1'] = (data_air.Nassau | data_air.Westchester)==1\n",
    "data_air['county1'].replace(False, 0, inplace=True)\n",
    "data_air['county1'] = data_air['county1'].astype(int)\n",
    "data_air['county2'] = (data_air.jfk | data_air.ewr | data_air.lgr | data_air.Nassau | data_air.Westchester)==0\n",
    "data_air['county2'].replace(False, 0, inplace=True)\n",
    "data_air['county2'] = data_air['county2'].astype(int)\n",
    "data = data[(data.jfk | data.ewr | data.lgr | data.Nassau | data.Suffolk | data.Westchester | data.Rockland | data.Dutchess | data.Orange | data.Putnam)==0]\n",
    "data_air.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "17624e0b3112e5507991a417f0ccbd520ba3cc20",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import seaborn as sns\\nimport matplotlib.pyplot as plt\\ncorrmat = data_air.corr()\\nf, ax = plt.subplots(figsize=(12, 9))\\n\\nk = 15 #number of variables for heatmap\\ncols = corrmat.nlargest(k, 'fare_amount')['fare_amount'].index\\ncm = np.corrcoef(data[cols].values.T)\\nsns.set(font_scale=1.25)\\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\\nplt.show()\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "corrmat = data_air.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "k = 15 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'fare_amount')['fare_amount'].index\n",
    "cm = np.corrcoef(data[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "47700a4563d5bfe42fc5e0956cb10c90c3dab29f",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f65187d3846d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#plt.xlabel('distance miles')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#plt.title('Histogram ride hour')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fare_amount'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#data.year_2015.hist(bins=50, figsize=(12,4))\n",
    "#plt.xlabel('distance miles')\n",
    "#plt.title('Histogram ride hour')\n",
    "plt.scatter(data['year'][:1000], data['fare_amount'][:1000])\n",
    "plt.show()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "c995ab67329370d8d459ff7668414a5385602c7d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>abs_diff_longitude</th>\n",
       "      <th>abs_diff_latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>Nassau</th>\n",
       "      <th>Suffolk</th>\n",
       "      <th>Westchester</th>\n",
       "      <th>Rockland</th>\n",
       "      <th>Dutchess</th>\n",
       "      <th>Orange</th>\n",
       "      <th>Putnam</th>\n",
       "      <th>airport</th>\n",
       "      <th>county1</th>\n",
       "      <th>county2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-06-15 17:26:21.0000001</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2009-06-15 17:26:21 UTC</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2009-01-09 16:10:00.000000170</td>\n",
       "      <td>31.90</td>\n",
       "      <td>2009-01-09 16:10:00 UTC</td>\n",
       "      <td>-73.873027</td>\n",
       "      <td>40.773883</td>\n",
       "      <td>-73.984545</td>\n",
       "      <td>40.769545</td>\n",
       "      <td>3</td>\n",
       "      <td>0.111518</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2010-05-31 18:06:00.00000097</td>\n",
       "      <td>34.67</td>\n",
       "      <td>2010-05-31 18:06:00 UTC</td>\n",
       "      <td>-73.985427</td>\n",
       "      <td>40.758853</td>\n",
       "      <td>-73.872907</td>\n",
       "      <td>40.774497</td>\n",
       "      <td>1</td>\n",
       "      <td>0.112520</td>\n",
       "      <td>0.015644</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2011-08-31 08:21:47.0000004</td>\n",
       "      <td>12.50</td>\n",
       "      <td>2011-08-31 08:21:47 UTC</td>\n",
       "      <td>-73.917399</td>\n",
       "      <td>40.746485</td>\n",
       "      <td>-73.973756</td>\n",
       "      <td>40.763836</td>\n",
       "      <td>1</td>\n",
       "      <td>0.056357</td>\n",
       "      <td>0.017351</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2012-05-11 15:03:53.0000001</td>\n",
       "      <td>32.50</td>\n",
       "      <td>2012-05-11 15:03:53 UTC</td>\n",
       "      <td>-73.872862</td>\n",
       "      <td>40.774105</td>\n",
       "      <td>-73.970593</td>\n",
       "      <td>40.764393</td>\n",
       "      <td>2</td>\n",
       "      <td>0.097731</td>\n",
       "      <td>0.009712</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               key  fare_amount          pickup_datetime  \\\n",
       "0      2009-06-15 17:26:21.0000001         4.50  2009-06-15 17:26:21 UTC   \n",
       "32   2009-01-09 16:10:00.000000170        31.90  2009-01-09 16:10:00 UTC   \n",
       "56    2010-05-31 18:06:00.00000097        34.67  2010-05-31 18:06:00 UTC   \n",
       "86     2011-08-31 08:21:47.0000004        12.50  2011-08-31 08:21:47 UTC   \n",
       "118    2012-05-11 15:03:53.0000001        32.50  2012-05-11 15:03:53 UTC   \n",
       "\n",
       "     pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "0          -73.844311        40.721319         -73.841610         40.712278   \n",
       "32         -73.873027        40.773883         -73.984545         40.769545   \n",
       "56         -73.985427        40.758853         -73.872907         40.774497   \n",
       "86         -73.917399        40.746485         -73.973756         40.763836   \n",
       "118        -73.872862        40.774105         -73.970593         40.764393   \n",
       "\n",
       "     passenger_count  abs_diff_longitude  abs_diff_latitude   ...    Nassau  \\\n",
       "0                  1            0.002701           0.009041   ...         1   \n",
       "32                 3            0.111518           0.004338   ...         1   \n",
       "56                 1            0.112520           0.015644   ...         1   \n",
       "86                 1            0.056357           0.017351   ...         1   \n",
       "118                2            0.097731           0.009712   ...         1   \n",
       "\n",
       "     Suffolk  Westchester  Rockland  Dutchess  Orange  Putnam  airport  \\\n",
       "0          0            0         0         0       0       0        0   \n",
       "32         0            0         0         0       0       0        1   \n",
       "56         0            0         0         0       0       0        1   \n",
       "86         0            0         0         0       0       0        0   \n",
       "118        0            0         0         0       0       0        1   \n",
       "\n",
       "     county1  county2  \n",
       "0          1        0  \n",
       "32         1        0  \n",
       "56         1        0  \n",
       "86         1        0  \n",
       "118        1        0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unwanted columns\n",
    "\n",
    "\n",
    "\n",
    "dropped_columns_air = ['day','pickup_longitude','pickup_latitude','dropoff_latitude','dropoff_longitude',\n",
    "                       'distance_to_center','passenger_count','Nassau','Westchester',\n",
    "                  'datetime_object','abs_diff_longitude','abs_diff_latitude','key','pickup_datetime']\n",
    "\n",
    "dropped_columns = ['day','pickup_longitude','pickup_latitude','dropoff_latitude','dropoff_longitude','distance_to_center',\n",
    "                  'datetime_object','abs_diff_longitude','abs_diff_latitude','key','pickup_datetime',\n",
    "                  'jfk','ewr','lgr', 'Nassau','Suffolk','Westchester','Rockland','Dutchess','Orange','Putnam'\n",
    "                  ]\n",
    "train_clean = data.drop(dropped_columns, axis=1)\n",
    "train_air_clean = data_air.drop(dropped_columns_air, axis=1)\n",
    "train_air_clean.head()\n",
    "data_air.head()\n",
    "#train_clean.head()\n",
    "#test_clean = test.drop(dropped_columns + ['key', 'passenger_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "521021ba33e6acb69ca70236837c7f4f33df431b"
   },
   "outputs": [],
   "source": [
    "# split data in train and validation (90% ~ 10%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, validation_df = train_test_split(train_clean, test_size=0.10, random_state=1)\n",
    "\n",
    "# Get labels\n",
    "train_labels = train_df['fare_amount'].values\n",
    "validation_labels = validation_df['fare_amount'].values\n",
    "train_df = train_df.drop(['fare_amount'], axis=1)\n",
    "validation_df = validation_df.drop(['fare_amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "a7bb2897578992fc5e9b298f7ebe93364ab329a8"
   },
   "outputs": [],
   "source": [
    "# split data in train and validation (90% ~ 10%)\n",
    "train_air_df, validation_air_df = train_test_split(train_air_clean, test_size=0.10, random_state=1)\n",
    "\n",
    "# Get labels\n",
    "train_air_labels = train_air_df['fare_amount'].values\n",
    "validation_air_labels = validation_air_df['fare_amount'].values\n",
    "train_air_df = train_air_df.drop(['fare_amount'], axis=1)\n",
    "validation_air_df = validation_air_df.drop(['fare_amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f8fb1d988ffe3f9c0edee12a7d2a0e56389eb6c9",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_miles</th>\n",
       "      <th>hour</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday</th>\n",
       "      <th>night</th>\n",
       "      <th>late_night</th>\n",
       "      <th>jfk</th>\n",
       "      <th>ewr</th>\n",
       "      <th>lgr</th>\n",
       "      <th>Suffolk</th>\n",
       "      <th>Rockland</th>\n",
       "      <th>Dutchess</th>\n",
       "      <th>Orange</th>\n",
       "      <th>Putnam</th>\n",
       "      <th>airport</th>\n",
       "      <th>county1</th>\n",
       "      <th>county2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.0</td>\n",
       "      <td>832.0</td>\n",
       "      <td>832.0</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.000000</td>\n",
       "      <td>832.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.234773</td>\n",
       "      <td>12.514423</td>\n",
       "      <td>2011.894231</td>\n",
       "      <td>2.805288</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>0.426683</td>\n",
       "      <td>0.263221</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.382212</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.651442</td>\n",
       "      <td>0.978365</td>\n",
       "      <td>0.008413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.047188</td>\n",
       "      <td>7.111272</td>\n",
       "      <td>1.876670</td>\n",
       "      <td>2.064092</td>\n",
       "      <td>0.402567</td>\n",
       "      <td>0.494893</td>\n",
       "      <td>0.440646</td>\n",
       "      <td>0.119299</td>\n",
       "      <td>0.486220</td>\n",
       "      <td>0.091393</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476800</td>\n",
       "      <td>0.145575</td>\n",
       "      <td>0.091393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.530840</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.212991</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.951679</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>62.134660</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       distance_miles        hour         year     weekday       night  \\\n",
       "count      832.000000  832.000000   832.000000  832.000000  832.000000   \n",
       "mean         7.234773   12.514423  2011.894231    2.805288    0.203125   \n",
       "std          5.047188    7.111272     1.876670    2.064092    0.402567   \n",
       "min          0.000000    0.000000  2009.000000    0.000000    0.000000   \n",
       "25%          4.530840    7.000000  2010.000000    1.000000    0.000000   \n",
       "50%          6.212991   14.000000  2012.000000    3.000000    0.000000   \n",
       "75%          9.951679   18.000000  2014.000000    5.000000    0.000000   \n",
       "max         62.134660   23.000000  2015.000000    6.000000    1.000000   \n",
       "\n",
       "       late_night         jfk         ewr         lgr     Suffolk    Rockland  \\\n",
       "count  832.000000  832.000000  832.000000  832.000000  832.000000  832.000000   \n",
       "mean     0.426683    0.263221    0.014423    0.382212    0.008413    0.002404   \n",
       "std      0.494893    0.440646    0.119299    0.486220    0.091393    0.049000   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000    1.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "       Dutchess  Orange  Putnam     airport     county1     county2  \n",
       "count     832.0   832.0   832.0  832.000000  832.000000  832.000000  \n",
       "mean        0.0     0.0     0.0    0.651442    0.978365    0.008413  \n",
       "std         0.0     0.0     0.0    0.476800    0.145575    0.091393  \n",
       "min         0.0     0.0     0.0    0.000000    0.000000    0.000000  \n",
       "25%         0.0     0.0     0.0    0.000000    1.000000    0.000000  \n",
       "50%         0.0     0.0     0.0    1.000000    1.000000    0.000000  \n",
       "75%         0.0     0.0     0.0    1.000000    1.000000    0.000000  \n",
       "max         0.0     0.0     0.0    1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df['distance_miles'] = distance(test_df.pickup_latitude, test_df.pickup_longitude, \\\n",
    "                                      test_df.dropoff_latitude, test_df.dropoff_longitude)\n",
    "test_df['datetime_object'] = [datetime.strptime(date,'%Y-%m-%d %H:%M:%S %Z') for date in test_df['pickup_datetime']]\n",
    "test_df['hour'] = [date.hour for date in test_df['datetime_object']]\n",
    "test_df['year'] = [date.year for date in test_df['datetime_object']]\n",
    "test_df['day'] = [date.day for date in test_df['datetime_object']]\n",
    "test_df['weekday'] = test_df['datetime_object'].apply(lambda x: x.weekday())\n",
    "test_df['night'] = test_df.apply (lambda x: night(x), axis=1)\n",
    "test_df['late_night'] = test_df.apply (lambda x: late_night(x), axis=1)\n",
    "\n",
    "#test_df['distance_to_center'] = distance(nyc[1], nyc[0],test_df.dropoff_latitude, test_df.dropoff_longitude)\n",
    "\n",
    "def add_checkpoint_test(point, point_name,rangeA):\n",
    "    test_df[point_name] = (distance(test_df.pickup_latitude, test_df.pickup_longitude, point[1], point[0]) <= rangeA) | ((distance(test_df.dropoff_latitude, test_df.dropoff_longitude, point[1], point[0]) <= rangeA))\n",
    "    test_df[point_name].replace(False, 0, inplace=True)\n",
    "    test_df[point_name] = test_df[point_name].astype(int)\n",
    "\n",
    "add_checkpoint_test(jfk, 'jfk',rangeA)\n",
    "add_checkpoint_test(ewr, 'ewr',rangeA)\n",
    "add_checkpoint_test(lgr, 'lgr',rangeA)\n",
    "add_checkpoint_test(Nassau, 'Nassau',rangeN)\n",
    "add_checkpoint_test(Suffolk, 'Suffolk',rangeS)\n",
    "add_checkpoint_test(Westchester, 'Westchester',rangeW)\n",
    "add_checkpoint_test(Rockland, 'Rockland',rangeR)\n",
    "add_checkpoint_test(Dutchess, 'Dutchess',rangeD)\n",
    "add_checkpoint_test(Orange, 'Orange',rangeO)\n",
    "add_checkpoint_test(Putnam, 'Putnam',rangeP)\n",
    "\n",
    "\n",
    "\n",
    "#test_df['euclidean'] = minkowski_distance(test_df['pickup_longitude'], test_df['dropoff_longitude'],\n",
    "#                                       test_df['pickup_latitude'], test_df['dropoff_latitude'], 2)\n",
    "\n",
    "test_air_df = test_df[(test_df.jfk | test_df.ewr | test_df.lgr | test_df.Nassau | test_df.Suffolk | test_df.Westchester | test_df.Rockland | test_df.Dutchess | test_df.Orange | test_df.Putnam)==1]\n",
    "test_df = test_df[(test_df.jfk | test_df.ewr | test_df.lgr | test_df.Nassau | test_df.Suffolk | test_df.Westchester | test_df.Rockland | test_df.Dutchess | test_df.Orange | test_df.Putnam)==0]\n",
    "\n",
    "dropped_columns_test = ['pickup_longitude', 'pickup_latitude', 'day','key',\n",
    "                        'jfk','ewr','lgr','Nassau','Suffolk','Westchester','Rockland','Dutchess','Orange','Putnam',\n",
    "                   'dropoff_longitude', 'dropoff_latitude' ,'datetime_object','pickup_datetime'\n",
    "                  ]\n",
    "test_clean = test_df.drop(dropped_columns_test, axis=1)\n",
    "test_clean.head()\n",
    "\n",
    "test_air_df['airport'] = (test_air_df.jfk | test_air_df.ewr | test_air_df.lgr )==1\n",
    "test_air_df['airport'].replace(False, 0, inplace=True)\n",
    "test_air_df['airport'] = test_air_df['airport'].astype(int)\n",
    "test_air_df['county1'] = (test_air_df.jfk | test_air_df.ewr | test_air_df.lgr )==0 \n",
    "test_air_df['county1'] = (test_air_df.Nassau | test_air_df.Westchester)==1\n",
    "test_air_df['county1'].replace(False, 0, inplace=True)\n",
    "test_air_df['county1'] = test_air_df['county1'].astype(int)\n",
    "test_air_df['county2'] = (test_air_df.jfk | test_air_df.ewr | test_air_df.lgr | test_air_df.Nassau | test_air_df.Westchester)==0\n",
    "test_air_df['county2'].replace(False, 0, inplace=True)\n",
    "test_air_df['county2'] = test_air_df['county2'].astype(int)\n",
    "\n",
    "\n",
    "dropped_columns_test_air = ['pickup_longitude', 'pickup_latitude', 'day','key','passenger_count','Nassau','Westchester',\n",
    "                   'dropoff_longitude', 'dropoff_latitude' ,'datetime_object','pickup_datetime'\n",
    "                  ]\n",
    "test_air_clean = test_air_df.drop(dropped_columns_test_air, axis=1)\n",
    "test_air_clean.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('data/train_df.csv')\n",
    "validation_df.to_csv('data/validation_df.csv')\n",
    "train_air_df.to_csv('data/train_air_df.csv')\n",
    "validation_air_df.to_csv('data/validation_air_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "d14dd4cd4418db0eb7784fa253fbd6aad3a22629",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ardroid\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "# Scale data\n",
    "# Note: im doing this here with sklearn scaler but, on the Coursera code the scaling is done with Dataflow and Tensorflow\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_df_scaled = scaler.fit_transform(train_df)\n",
    "validation_df_scaled = scaler.transform(validation_df)\n",
    "test_scaled = scaler.transform(test_clean)\n",
    "\n",
    "train_air_df_scaled = scaler.fit_transform(train_air_df)\n",
    "validation_air_df_scaled = scaler.transform(validation_air_df)\n",
    "test_air_scaled = scaler.transform(test_air_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b7ac029887665e1615477d1dd53909b7b8bd4b2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "DATASET_SIZE = 6000000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_dim=train_df_scaled.shape[1], activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1))\n",
    "\n",
    "adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "model.compile(loss='mse', optimizer=adam, metrics=['mae'])\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ec3f622b3dd6541f15603423fed272419e5ce52",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "DATASET_SIZE = 6000000\n",
    "\n",
    "model_air = Sequential()\n",
    "model_air.add(Dense(256, activation='relu', input_dim=train_air_df_scaled.shape[1], activity_regularizer=regularizers.l1(0.01)))\n",
    "model_air.add(BatchNormalization())\n",
    "model_air.add(Dense(128, activation='relu'))\n",
    "model_air.add(BatchNormalization())\n",
    "model_air.add(Dense(64, activation='relu'))\n",
    "model_air.add(BatchNormalization())\n",
    "model_air.add(Dense(32, activation='relu'))\n",
    "model_air.add(BatchNormalization())\n",
    "model_air.add(Dense(16, activation='relu'))\n",
    "model_air.add(BatchNormalization())\n",
    "model_air.add(Dense(1))\n",
    "\n",
    "adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "model_air.compile(loss='mse', optimizer=adam, metrics=['mae'])\n",
    "print(train_air_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f852870068ff992bd974bb7bf9fa5c582ffe49a5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Dataset size: %s' % DATASET_SIZE)\n",
    "print('Epochs: %s' % EPOCHS)\n",
    "print('Learning rate: %s' % LEARNING_RATE)\n",
    "print('Batch size: %s' % BATCH_SIZE)\n",
    "print('Input dimension: %s' % train_df_scaled.shape[1])\n",
    "print('Features used: %s' % train_df.columns)\n",
    "model.summary()\n",
    "history = model.fit(x=train_df_scaled, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                    verbose=1, validation_data=(validation_df_scaled, validation_labels), \n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bdca0dc697498768f6e18c8f5df9e7f7b65f386",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Dataset size: %s' % DATASET_SIZE)\n",
    "print('Epochs: %s' % EPOCHS)\n",
    "print('Learning rate: %s' % LEARNING_RATE)\n",
    "print('Batch size: %s' % BATCH_SIZE)\n",
    "print('Input dimension: %s' % train_air_df_scaled.shape[1])\n",
    "print('Features used: %s' % train_air_df.columns)\n",
    "model_air.summary()\n",
    "history_air = model_air.fit(x=train_air_df_scaled, y=train_air_labels, batch_size=BATCH_SIZE, epochs=EPOCHS*2, \n",
    "                    verbose=1, validation_data=(validation_air_df_scaled, validation_air_labels), \n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dd3285487937f33f4f7bb4b77517319498e036c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_loss_accuracy(history)\n",
    "\n",
    "SUBMISSION_NAME = 'submission.csv'\n",
    "def output_submission(raw_test,prediction,  file_name):\n",
    "    df = pd.DataFrame(prediction, columns=['fare_amount'])\n",
    "    df['key'] = raw_test['key']\n",
    "                  \n",
    "    #raw_test = raw_test.drop(dropped_columns, axis=1)\n",
    "    df[['key','fare_amount']].to_csv((file_name), index=False)\n",
    "    \n",
    "    #print(df)\n",
    "    print('Output complete')\n",
    "    print(df)\n",
    "    \n",
    "prediction = model.predict(test_scaled, batch_size=128, verbose=1)\n",
    "prediction_air = model_air.predict(test_air_scaled, batch_size=128, verbose=1)\n",
    "#prediction_air = model_air.predict(test_air_scaled, num_iteration = model_air.best_iteration)\n",
    "#print(prediction_air)\n",
    "frames = [test_df, test_air_df]\n",
    "test_final = pd.concat(frames)\n",
    "frames = [prediction, prediction_air]\n",
    "prediction_final = np.concatenate(frames)\n",
    "\n",
    "\n",
    "#test_df = pd.read_csv('../input/test.csv')\n",
    "result=dict()\n",
    "print(len(test_final))\n",
    "i=0\n",
    "#print(test_df[0])\n",
    "for index, row in test_final.iterrows():\n",
    "    result[row['key']]=prediction_final[i]\n",
    "    i=i+1\n",
    "test_df1 = pd.read_csv('../input/test.csv')\n",
    "\n",
    "pred=[]\n",
    "for index, row in test_df1.iterrows():\n",
    "    pred.append(result[row['key']])\n",
    "    \n",
    "test_df1.head()\n",
    "output_submission(test_df1,pred, SUBMISSION_NAME)\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#y = data.fare_amount\n",
    "#X = data.drop('fare_amount', axis=1)\n",
    "#train_df, val_df, train_y, val_y = train_test_split(X, y,test_size=0.2)\n",
    "#train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f2c800f5a6a167ee32508b65479c808044aa171"
   },
   "outputs": [],
   "source": [
    "# Construct and return an Nx3 input matrix for our linear model\n",
    "# using the travel vector, plus a 1.0 for a constant bias term.\n",
    "def get_input_matrix(df):\n",
    "    return np.column_stack((df.distance_miles, df.passenger_count,df.hour,df.year, np.ones(len(df))))\n",
    "\n",
    "#train_X = get_input_matrix(train_df)\n",
    "#train_y = np.array(train_df['fare_amount'])\n",
    "\n",
    "#print(train_X.shape)\n",
    "#print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45b8b93121272d9e53b87c65656e4664189119f5"
   },
   "outputs": [],
   "source": [
    "# The lstsq function returns several things, and we only care about the actual weight vector w.\n",
    "#(w, _, _, _) = np.linalg.lstsq(train_X, train_y, rcond = None)\n",
    "#print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c013ad4ef4170783901cd43f8ec7460b092bb782"
   },
   "outputs": [],
   "source": [
    "#w_OLS = np.matmul(np.matmul(np.linalg.inv(np.matmul(train_X.T, train_X)), train_X.T), train_y)\n",
    "#print(w_OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72c92552f4eab983aec23cb227ac2e454b66d8e0"
   },
   "outputs": [],
   "source": [
    "#test_df = pd.read_csv('../input/test.csv')\n",
    "#test_df['distance_miles'] = distance(test_df.pickup_latitude, test_df.pickup_longitude, \\\n",
    "                                      test_df.dropoff_latitude, test_df.dropoff_longitude)\n",
    "#test_df['datetime_object'] = [datetime.strptime(date,'%Y-%m-%d %H:%M:%S %Z') for date in test_df['pickup_datetime']]\n",
    "#test_df['hour'] = [date.hour for date in test_df['datetime_object']]\n",
    "#test_df['year'] = [date.year for date in test_df['datetime_object']]\n",
    "\n",
    "#val_df['distance_miles'] = distance(val_df.pickup_latitude, val_df.pickup_longitude, \\\n",
    "                                      val_df.dropoff_latitude, val_df.dropoff_longitude)\n",
    "#val_df['datetime_object'] = [datetime.strptime(date,'%Y-%m-%d %H:%M:%S %Z') for date in val_df['pickup_datetime']]\n",
    "#val_df['hour'] = [date.hour for date in val_df['datetime_object']]\n",
    "#val_df['year'] = [date.year for date in val_df['datetime_object']]\n",
    "test_df.dtypes\n",
    "#val_df = pd.read_csv('../input/train.csv', nrows = 10000000)\n",
    "#val_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e94990202009366436f78fdd66d546f40d4e85d"
   },
   "outputs": [],
   "source": [
    "# Reuse the above helper functions to add our features and generate the input matrix.\n",
    "#add_travel_vector_features(test_df)\n",
    "#test_X = get_input_matrix(test_df)\n",
    "#add_travel_vector_features(val_df)\n",
    "#val_df = val_df.dropna(how = 'any', axis = 'rows')\n",
    "#val_df = val_df[(val_df.abs_diff_longitude < 5.0) & (val_df.abs_diff_latitude < 5.0)]\n",
    "#val_X = get_input_matrix(val_df)\n",
    "# Predict fare_amount on the test set using our model (w) trained on the training set.\n",
    "#test_y_predictions = np.matmul(test_X, w).round(decimals = 2)\n",
    "#val_y_predictions = np.matmul(val_X, w).round(decimals = 2)\n",
    "#val_y = np.array(val_df['fare_amount'])\n",
    "\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#print(np.sqrt(mean_squared_error(val_y, val_y_predictions)))\n",
    "# Write the predictions to a CSV file which we can submit to the competition.\n",
    "#submission = pd.DataFrame(\n",
    "#    {'key': test_df.key, 'fare_amount': test_y_predictions},\n",
    "#    columns = ['key', 'fare_amount'])\n",
    "#submission.to_csv('submission.csv', index = False)\n",
    "\n",
    "#print(os.listdir('.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
