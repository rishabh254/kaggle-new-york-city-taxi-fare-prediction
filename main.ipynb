{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "a9e598214dce2f7aff9389c58f6871a8506966d8"
   },
   "outputs": [],
   "source": [
    "data =  pd.read_csv('data/train.csv', nrows = 15_000_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Given a dataframe, add two new features 'abs_diff_longitude' and\n",
    "# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n",
    "# the pickup location to the dropoff location.\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "add_travel_vector_features(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "3501b1a8a57342293c6ab6f3e5779858b9cf60ff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old size: 15000000\n",
      "New size: 14999901\n"
     ]
    }
   ],
   "source": [
    "print('Old size: %d' % len(data))\n",
    "data = data.dropna(how = 'any', axis = 'rows')\n",
    "print('New size: %d' % len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "c51a15f4a365862f62dea455f4c2462d2190ba4e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "data['datetime_object'] = [datetime.strptime(date,'%Y-%m-%d %H:%M:%S %Z') for date in data['pickup_datetime']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "4fc9f4307938fa4acf3a91b5a54363c14357d501",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>abs_diff_longitude</th>\n",
       "      <th>abs_diff_latitude</th>\n",
       "      <th>datetime_object</th>\n",
       "      <th>distance_miles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-06-15 17:26:21.0000001</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2009-06-15 17:26:21 UTC</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>2009-06-15 17:26:21</td>\n",
       "      <td>0.640487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05 16:52:16.0000002</td>\n",
       "      <td>16.9</td>\n",
       "      <td>2010-01-05 16:52:16 UTC</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036780</td>\n",
       "      <td>0.070701</td>\n",
       "      <td>2010-01-05 16:52:16</td>\n",
       "      <td>5.250670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-08-18 00:35:00.00000049</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2011-08-18 00:35:00 UTC</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>2</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>2011-08-18 00:35:00</td>\n",
       "      <td>0.863411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-04-21 04:30:42.0000001</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2012-04-21 04:30:42 UTC</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.024949</td>\n",
       "      <td>2012-04-21 04:30:42</td>\n",
       "      <td>1.739386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-03-09 07:51:00.000000135</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2010-03-09 07:51:00 UTC</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>0.015754</td>\n",
       "      <td>2010-03-09 07:51:00</td>\n",
       "      <td>1.242218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             key  fare_amount          pickup_datetime  \\\n",
       "0    2009-06-15 17:26:21.0000001          4.5  2009-06-15 17:26:21 UTC   \n",
       "1    2010-01-05 16:52:16.0000002         16.9  2010-01-05 16:52:16 UTC   \n",
       "2   2011-08-18 00:35:00.00000049          5.7  2011-08-18 00:35:00 UTC   \n",
       "3    2012-04-21 04:30:42.0000001          7.7  2012-04-21 04:30:42 UTC   \n",
       "4  2010-03-09 07:51:00.000000135          5.3  2010-03-09 07:51:00 UTC   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "0        -73.844311        40.721319         -73.841610         40.712278   \n",
       "1        -74.016048        40.711303         -73.979268         40.782004   \n",
       "2        -73.982738        40.761270         -73.991242         40.750562   \n",
       "3        -73.987130        40.733143         -73.991567         40.758092   \n",
       "4        -73.968095        40.768008         -73.956655         40.783762   \n",
       "\n",
       "   passenger_count  abs_diff_longitude  abs_diff_latitude     datetime_object  \\\n",
       "0                1            0.002701           0.009041 2009-06-15 17:26:21   \n",
       "1                1            0.036780           0.070701 2010-01-05 16:52:16   \n",
       "2                2            0.008504           0.010708 2011-08-18 00:35:00   \n",
       "3                1            0.004437           0.024949 2012-04-21 04:30:42   \n",
       "4                1            0.011440           0.015754 2010-03-09 07:51:00   \n",
       "\n",
       "   distance_miles  \n",
       "0        0.640487  \n",
       "1        5.250670  \n",
       "2        0.863411  \n",
       "3        1.739386  \n",
       "4        1.242218  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(data.describe())\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = 0.017453292519943295 # Pi/180\n",
    "    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n",
    "    return 0.6213712 * 12742 * np.arcsin(np.sqrt(a)) # 2*R*asin...\n",
    "\n",
    "# add new column to dataframe with distance in miles\n",
    "data['distance_miles'] = distance(data.pickup_latitude, data.pickup_longitude, \\\n",
    "                                      data.dropoff_latitude, data.dropoff_longitude)\n",
    "\n",
    "(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1dc7077a9940f07baf0cdcda99f2c01d991894b9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old size: 14999901\n"
     ]
    }
   ],
   "source": [
    "print('Old size: %d' % len(data))\n",
    "data = data[(data.abs_diff_longitude < 3.0) & (data.abs_diff_latitude < 3.0)]\n",
    "data = data[(data.pickup_longitude >= -74.3) & (data.pickup_longitude <= -72.9)]  # nyc coordinates\n",
    "data = data[(data.dropoff_longitude >= -74.3) & (data.dropoff_longitude <= -72.9)]\n",
    "data = data[(data.pickup_latitude >= 40.5) & (data.pickup_latitude <= 41.8)]\n",
    "data = data[(data.dropoff_latitude >= 40.5) & (data.dropoff_latitude <= 41.8)]\n",
    "data = data[(data.fare_amount>=2) & (data.fare_amount<=500)]\n",
    "data = data[(data.passenger_count>0) & (data.passenger_count <=6)]\n",
    "data = data[(data.distance_miles<=100.0) & (data.distance_miles>0.05)]\n",
    "nyc = (-74.0063889, 40.7141667)\n",
    "data['distance_to_center'] = distance(nyc[1], nyc[0],data.dropoff_latitude, data.dropoff_longitude)\n",
    "data = data[data.distance_to_center<15.0]\n",
    "print('New size: %d' % len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0587baf20567e6855e9a8342646eb3cc607b6cda"
   },
   "outputs": [],
   "source": [
    "def late_night (row):\n",
    "    if (row['hour'] <= 6) or (row['hour'] >= 20):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def night (row):\n",
    "    if ((row['hour'] <= 20) and (row['hour'] >= 16)) and (row['weekday'] < 5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "\n",
    "#data.distance_miles.hist(bins=50, figsize=(12,4))\n",
    "#plt.xlabel('distance miles')\n",
    "#plt.title('Histogram ride distances in miles')\n",
    "#data.groupby('passenger_count')['distance_miles', 'fare_amount'].mean()\n",
    "#print(\"Average $USD/Mile : {:0.2f}\".format(data.fare_amount.sum()/data.distance_miles.sum()))\n",
    "#data['fare_per_mile'] = data.fare_amount / data.distance_miles\n",
    "data['hour'] = [date.hour for date in data['datetime_object']]\n",
    "data['year'] = [date.year for date in data['datetime_object']]\n",
    "data['day'] = [date.day for date in data['datetime_object']]\n",
    "data['weekday'] = data['datetime_object'].apply(lambda x: x.weekday())\n",
    "data['night'] = data.apply (lambda x: night(x), axis=1)\n",
    "data['late_night'] = data.apply (lambda x: late_night(x), axis=1)   \n",
    "# There is a $1 surcharge from 4pm to 8pm on weekdays, excluding holidays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c647154ffae4423a6626432c846c8fed188e9cb5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rangeA = 1.5\n",
    "rangeN = 20.0\n",
    "rangeS = 48.7\n",
    "rangeR = 14.1\n",
    "rangeD = 28.7\n",
    "rangeO = 29\n",
    "rangeP = 15.7\n",
    "rangeW = 20.0\n",
    "jfk = (-73.7822222222, 40.6441666667) #JFK Airport\n",
    "ewr = (-74.175, 40.69) # Newark Liberty International Airport\n",
    "lgr = (-73.87, 40.77) # LaGuardia Airport\n",
    "\n",
    " # county\n",
    "Nassau = (-73.5594, 40.6546)\n",
    "Suffolk = (-72.6151, 40.9849)\n",
    "Westchester = (-73.7949, 41.1220)\n",
    "Rockland = (-73.9830, 41.1489)\n",
    "Dutchess = (-73.7478, 41.7784)\n",
    "Orange = (-74.3118, 41.3912)\n",
    "Putnam = (-73.7949, 41.4351) \n",
    "\n",
    "data_air=data\n",
    "\n",
    "def add_checkpoint(point, point_name,rangeA):\n",
    "    data_air[point_name] = (distance(data.pickup_latitude, data.pickup_longitude, point[1], point[0]) <= rangeA) | ((distance(data.dropoff_latitude, data.dropoff_longitude, point[1], point[0]) <= rangeA))\n",
    "    data_air[point_name].replace(False, 0, inplace=True)\n",
    "    data_air[point_name] = data_air[point_name].astype(int)\n",
    "\n",
    "add_checkpoint(jfk, 'jfk',rangeA)\n",
    "add_checkpoint(ewr, 'ewr',rangeA)\n",
    "add_checkpoint(lgr, 'lgr',rangeA)\n",
    "add_checkpoint(Nassau, 'Nassau',rangeN)\n",
    "add_checkpoint(Suffolk, 'Suffolk',rangeS)\n",
    "add_checkpoint(Westchester, 'Westchester',rangeW)\n",
    "add_checkpoint(Rockland, 'Rockland',rangeR )\n",
    "add_checkpoint(Dutchess, 'Dutchess',rangeD)\n",
    "add_checkpoint(Orange, 'Orange',rangeO)\n",
    "add_checkpoint(Putnam, 'Putnam',rangeP)\n",
    "\n",
    "data_air = data[(data_air.jfk | data_air.ewr | data_air.lgr | data_air.Nassau | data_air.Suffolk | data_air.Westchester | data_air.Rockland | data_air.Dutchess | data_air.Orange | data_air.Putnam)==1]\n",
    "data_air['airport'] = (data_air.jfk | data_air.ewr | data_air.lgr )==1\n",
    "data_air['airport'].replace(False, 0, inplace=True)\n",
    "data_air['airport'] = data_air['airport'].astype(int)\n",
    "data_air['county1'] = (data_air.jfk | data_air.ewr | data_air.lgr )==0\n",
    "data_air['county1'] = (data_air.Nassau | data_air.Westchester)==1\n",
    "data_air['county1'].replace(False, 0, inplace=True)\n",
    "data_air['county1'] = data_air['county1'].astype(int)\n",
    "data_air['county2'] = (data_air.jfk | data_air.ewr | data_air.lgr | data_air.Nassau | data_air.Westchester)==0\n",
    "data_air['county2'].replace(False, 0, inplace=True)\n",
    "data_air['county2'] = data_air['county2'].astype(int)\n",
    "data = data[(data.jfk | data.ewr | data.lgr | data.Nassau | data.Suffolk | data.Westchester | data.Rockland | data.Dutchess | data.Orange | data.Putnam)==0]\n",
    "data_air.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17624e0b3112e5507991a417f0ccbd520ba3cc20",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "corrmat = data_air.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "k = 15 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'fare_amount')['fare_amount'].index\n",
    "cm = np.corrcoef(data[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47700a4563d5bfe42fc5e0956cb10c90c3dab29f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#data.year_2015.hist(bins=50, figsize=(12,4))\n",
    "#plt.xlabel('distance miles')\n",
    "#plt.title('Histogram ride hour')\n",
    "plt.scatter(data['year'][:1000], data['fare_amount'][:1000])\n",
    "plt.show()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c995ab67329370d8d459ff7668414a5385602c7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "\n",
    "\n",
    "\n",
    "dropped_columns_air = ['day','pickup_longitude','pickup_latitude','dropoff_latitude','dropoff_longitude',\n",
    "                       'distance_to_center','passenger_count','Nassau','Westchester',\n",
    "                  'datetime_object','abs_diff_longitude','abs_diff_latitude','key','pickup_datetime']\n",
    "\n",
    "dropped_columns = ['day','pickup_longitude','pickup_latitude','dropoff_latitude','dropoff_longitude','distance_to_center',\n",
    "                  'datetime_object','abs_diff_longitude','abs_diff_latitude','key','pickup_datetime',\n",
    "                  'jfk','ewr','lgr', 'Nassau','Suffolk','Westchester','Rockland','Dutchess','Orange','Putnam'\n",
    "                  ]\n",
    "train_clean = data.drop(dropped_columns, axis=1)\n",
    "train_air_clean = data_air.drop(dropped_columns_air, axis=1)\n",
    "train_air_clean.head()\n",
    "data_air.head()\n",
    "#train_clean.head()\n",
    "#test_clean = test.drop(dropped_columns + ['key', 'passenger_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "521021ba33e6acb69ca70236837c7f4f33df431b"
   },
   "outputs": [],
   "source": [
    "# split data in train and validation (90% ~ 10%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, validation_df = train_test_split(train_clean, test_size=0.10, random_state=1)\n",
    "\n",
    "# Get labels\n",
    "train_labels = train_df['fare_amount'].values\n",
    "validation_labels = validation_df['fare_amount'].values\n",
    "train_df = train_df.drop(['fare_amount'], axis=1)\n",
    "validation_df = validation_df.drop(['fare_amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7bb2897578992fc5e9b298f7ebe93364ab329a8"
   },
   "outputs": [],
   "source": [
    "# split data in train and validation (90% ~ 10%)\n",
    "train_air_df, validation_air_df = train_test_split(train_air_clean, test_size=0.10, random_state=1)\n",
    "\n",
    "# Get labels\n",
    "train_air_labels = train_air_df['fare_amount'].values\n",
    "validation_air_labels = validation_air_df['fare_amount'].values\n",
    "train_air_df = train_air_df.drop(['fare_amount'], axis=1)\n",
    "validation_air_df = validation_air_df.drop(['fare_amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8fb1d988ffe3f9c0edee12a7d2a0e56389eb6c9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df['distance_miles'] = distance(test_df.pickup_latitude, test_df.pickup_longitude, \\\n",
    "                                      test_df.dropoff_latitude, test_df.dropoff_longitude)\n",
    "test_df['datetime_object'] = [datetime.strptime(date,'%Y-%m-%d %H:%M:%S %Z') for date in test_df['pickup_datetime']]\n",
    "test_df['hour'] = [date.hour for date in test_df['datetime_object']]\n",
    "test_df['year'] = [date.year for date in test_df['datetime_object']]\n",
    "test_df['day'] = [date.day for date in test_df['datetime_object']]\n",
    "test_df['weekday'] = test_df['datetime_object'].apply(lambda x: x.weekday())\n",
    "test_df['night'] = test_df.apply (lambda x: night(x), axis=1)\n",
    "test_df['late_night'] = test_df.apply (lambda x: late_night(x), axis=1)\n",
    "\n",
    "#test_df['distance_to_center'] = distance(nyc[1], nyc[0],test_df.dropoff_latitude, test_df.dropoff_longitude)\n",
    "\n",
    "def add_checkpoint_test(point, point_name,rangeA):\n",
    "    test_df[point_name] = (distance(test_df.pickup_latitude, test_df.pickup_longitude, point[1], point[0]) <= rangeA) | ((distance(test_df.dropoff_latitude, test_df.dropoff_longitude, point[1], point[0]) <= rangeA))\n",
    "    test_df[point_name].replace(False, 0, inplace=True)\n",
    "    test_df[point_name] = test_df[point_name].astype(int)\n",
    "\n",
    "add_checkpoint_test(jfk, 'jfk',rangeA)\n",
    "add_checkpoint_test(ewr, 'ewr',rangeA)\n",
    "add_checkpoint_test(lgr, 'lgr',rangeA)\n",
    "add_checkpoint_test(Nassau, 'Nassau',rangeN)\n",
    "add_checkpoint_test(Suffolk, 'Suffolk',rangeS)\n",
    "add_checkpoint_test(Westchester, 'Westchester',rangeW)\n",
    "add_checkpoint_test(Rockland, 'Rockland',rangeR)\n",
    "add_checkpoint_test(Dutchess, 'Dutchess',rangeD)\n",
    "add_checkpoint_test(Orange, 'Orange',rangeO)\n",
    "add_checkpoint_test(Putnam, 'Putnam',rangeP)\n",
    "\n",
    "\n",
    "\n",
    "#test_df['euclidean'] = minkowski_distance(test_df['pickup_longitude'], test_df['dropoff_longitude'],\n",
    "#                                       test_df['pickup_latitude'], test_df['dropoff_latitude'], 2)\n",
    "\n",
    "test_air_df = test_df[(test_df.jfk | test_df.ewr | test_df.lgr | test_df.Nassau | test_df.Suffolk | test_df.Westchester | test_df.Rockland | test_df.Dutchess | test_df.Orange | test_df.Putnam)==1]\n",
    "test_df = test_df[(test_df.jfk | test_df.ewr | test_df.lgr | test_df.Nassau | test_df.Suffolk | test_df.Westchester | test_df.Rockland | test_df.Dutchess | test_df.Orange | test_df.Putnam)==0]\n",
    "\n",
    "dropped_columns_test = ['pickup_longitude', 'pickup_latitude', 'day','key',\n",
    "                        'jfk','ewr','lgr','Nassau','Suffolk','Westchester','Rockland','Dutchess','Orange','Putnam',\n",
    "                   'dropoff_longitude', 'dropoff_latitude' ,'datetime_object','pickup_datetime'\n",
    "                  ]\n",
    "test_clean = test_df.drop(dropped_columns_test, axis=1)\n",
    "test_clean.head()\n",
    "\n",
    "test_air_df['airport'] = (test_air_df.jfk | test_air_df.ewr | test_air_df.lgr )==1\n",
    "test_air_df['airport'].replace(False, 0, inplace=True)\n",
    "test_air_df['airport'] = test_air_df['airport'].astype(int)\n",
    "test_air_df['county1'] = (test_air_df.jfk | test_air_df.ewr | test_air_df.lgr )==0 \n",
    "test_air_df['county1'] = (test_air_df.Nassau | test_air_df.Westchester)==1\n",
    "test_air_df['county1'].replace(False, 0, inplace=True)\n",
    "test_air_df['county1'] = test_air_df['county1'].astype(int)\n",
    "test_air_df['county2'] = (test_air_df.jfk | test_air_df.ewr | test_air_df.lgr | test_air_df.Nassau | test_air_df.Westchester)==0\n",
    "test_air_df['county2'].replace(False, 0, inplace=True)\n",
    "test_air_df['county2'] = test_air_df['county2'].astype(int)\n",
    "\n",
    "\n",
    "dropped_columns_test_air = ['pickup_longitude', 'pickup_latitude', 'day','key','passenger_count','Nassau','Westchester',\n",
    "                   'dropoff_longitude', 'dropoff_latitude' ,'datetime_object','pickup_datetime'\n",
    "                  ]\n",
    "test_air_clean = test_air_df.drop(dropped_columns_test_air, axis=1)\n",
    "test_air_clean.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('data/train_df.csv')\n",
    "validation_df.to_csv('data/validation_df.csv')\n",
    "train_air_df.to_csv('data/train_air_df.csv')\n",
    "validation_air_df.to_csv('data/validation_air_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d14dd4cd4418db0eb7784fa253fbd6aad3a22629",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "# Scale data\n",
    "# Note: im doing this here with sklearn scaler but, on the Coursera code the scaling is done with Dataflow and Tensorflow\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_df_scaled = scaler.fit_transform(train_df)\n",
    "validation_df_scaled = scaler.transform(validation_df)\n",
    "test_scaled = scaler.transform(test_clean)\n",
    "\n",
    "train_air_df_scaled = scaler.fit_transform(train_air_df)\n",
    "validation_air_df_scaled = scaler.transform(validation_air_df)\n",
    "test_air_scaled = scaler.transform(test_air_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b7ac029887665e1615477d1dd53909b7b8bd4b2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "DATASET_SIZE = 6000000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_dim=train_df_scaled.shape[1], activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1))\n",
    "\n",
    "adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "model.compile(loss='mse', optimizer=adam, metrics=['mae'])\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ec3f622b3dd6541f15603423fed272419e5ce52",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "DATASET_SIZE = 6000000\n",
    "\n",
    "model_air = Sequential()\n",
    "model_air.add(Dense(256, activation='relu', input_dim=train_air_df_scaled.shape[1], activity_regularizer=regularizers.l1(0.01)))\n",
    "model_air.add(BatchNormalization())\n",
    "model_air.add(Dense(128, activation='relu'))\n",
    "model_air.add(BatchNormalization())\n",
    "model_air.add(Dense(64, activation='relu'))\n",
    "model_air.add(BatchNormalization())\n",
    "model_air.add(Dense(32, activation='relu'))\n",
    "model_air.add(BatchNormalization())\n",
    "model_air.add(Dense(16, activation='relu'))\n",
    "model_air.add(BatchNormalization())\n",
    "model_air.add(Dense(1))\n",
    "\n",
    "adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "model_air.compile(loss='mse', optimizer=adam, metrics=['mae'])\n",
    "print(train_air_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f852870068ff992bd974bb7bf9fa5c582ffe49a5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Dataset size: %s' % DATASET_SIZE)\n",
    "print('Epochs: %s' % EPOCHS)\n",
    "print('Learning rate: %s' % LEARNING_RATE)\n",
    "print('Batch size: %s' % BATCH_SIZE)\n",
    "print('Input dimension: %s' % train_df_scaled.shape[1])\n",
    "print('Features used: %s' % train_df.columns)\n",
    "model.summary()\n",
    "history = model.fit(x=train_df_scaled, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                    verbose=1, validation_data=(validation_df_scaled, validation_labels), \n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bdca0dc697498768f6e18c8f5df9e7f7b65f386",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Dataset size: %s' % DATASET_SIZE)\n",
    "print('Epochs: %s' % EPOCHS)\n",
    "print('Learning rate: %s' % LEARNING_RATE)\n",
    "print('Batch size: %s' % BATCH_SIZE)\n",
    "print('Input dimension: %s' % train_air_df_scaled.shape[1])\n",
    "print('Features used: %s' % train_air_df.columns)\n",
    "model_air.summary()\n",
    "history_air = model_air.fit(x=train_air_df_scaled, y=train_air_labels, batch_size=BATCH_SIZE, epochs=EPOCHS*2, \n",
    "                    verbose=1, validation_data=(validation_air_df_scaled, validation_air_labels), \n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dd3285487937f33f4f7bb4b77517319498e036c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_loss_accuracy(history)\n",
    "\n",
    "SUBMISSION_NAME = 'submission.csv'\n",
    "def output_submission(raw_test,prediction,  file_name):\n",
    "    df = pd.DataFrame(prediction, columns=['fare_amount'])\n",
    "    df['key'] = raw_test['key']\n",
    "                  \n",
    "    #raw_test = raw_test.drop(dropped_columns, axis=1)\n",
    "    df[['key','fare_amount']].to_csv((file_name), index=False)\n",
    "    \n",
    "    #print(df)\n",
    "    print('Output complete')\n",
    "    print(df)\n",
    "    \n",
    "prediction = model.predict(test_scaled, batch_size=128, verbose=1)\n",
    "prediction_air = model_air.predict(test_air_scaled, batch_size=128, verbose=1)\n",
    "#prediction_air = model_air.predict(test_air_scaled, num_iteration = model_air.best_iteration)\n",
    "#print(prediction_air)\n",
    "frames = [test_df, test_air_df]\n",
    "test_final = pd.concat(frames)\n",
    "frames = [prediction, prediction_air]\n",
    "prediction_final = np.concatenate(frames)\n",
    "\n",
    "\n",
    "#test_df = pd.read_csv('../input/test.csv')\n",
    "result=dict()\n",
    "print(len(test_final))\n",
    "i=0\n",
    "#print(test_df[0])\n",
    "for index, row in test_final.iterrows():\n",
    "    result[row['key']]=prediction_final[i]\n",
    "    i=i+1\n",
    "test_df1 = pd.read_csv('../input/test.csv')\n",
    "\n",
    "pred=[]\n",
    "for index, row in test_df1.iterrows():\n",
    "    pred.append(result[row['key']])\n",
    "    \n",
    "test_df1.head()\n",
    "output_submission(test_df1,pred, SUBMISSION_NAME)\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#y = data.fare_amount\n",
    "#X = data.drop('fare_amount', axis=1)\n",
    "#train_df, val_df, train_y, val_y = train_test_split(X, y,test_size=0.2)\n",
    "#train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f2c800f5a6a167ee32508b65479c808044aa171"
   },
   "outputs": [],
   "source": [
    "# Construct and return an Nx3 input matrix for our linear model\n",
    "# using the travel vector, plus a 1.0 for a constant bias term.\n",
    "def get_input_matrix(df):\n",
    "    return np.column_stack((df.distance_miles, df.passenger_count,df.hour,df.year, np.ones(len(df))))\n",
    "\n",
    "#train_X = get_input_matrix(train_df)\n",
    "#train_y = np.array(train_df['fare_amount'])\n",
    "\n",
    "#print(train_X.shape)\n",
    "#print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45b8b93121272d9e53b87c65656e4664189119f5"
   },
   "outputs": [],
   "source": [
    "# The lstsq function returns several things, and we only care about the actual weight vector w.\n",
    "#(w, _, _, _) = np.linalg.lstsq(train_X, train_y, rcond = None)\n",
    "#print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c013ad4ef4170783901cd43f8ec7460b092bb782"
   },
   "outputs": [],
   "source": [
    "#w_OLS = np.matmul(np.matmul(np.linalg.inv(np.matmul(train_X.T, train_X)), train_X.T), train_y)\n",
    "#print(w_OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72c92552f4eab983aec23cb227ac2e454b66d8e0"
   },
   "outputs": [],
   "source": [
    "#test_df = pd.read_csv('../input/test.csv')\n",
    "#test_df['distance_miles'] = distance(test_df.pickup_latitude, test_df.pickup_longitude, \\\n",
    "                                      test_df.dropoff_latitude, test_df.dropoff_longitude)\n",
    "#test_df['datetime_object'] = [datetime.strptime(date,'%Y-%m-%d %H:%M:%S %Z') for date in test_df['pickup_datetime']]\n",
    "#test_df['hour'] = [date.hour for date in test_df['datetime_object']]\n",
    "#test_df['year'] = [date.year for date in test_df['datetime_object']]\n",
    "\n",
    "#val_df['distance_miles'] = distance(val_df.pickup_latitude, val_df.pickup_longitude, \\\n",
    "                                      val_df.dropoff_latitude, val_df.dropoff_longitude)\n",
    "#val_df['datetime_object'] = [datetime.strptime(date,'%Y-%m-%d %H:%M:%S %Z') for date in val_df['pickup_datetime']]\n",
    "#val_df['hour'] = [date.hour for date in val_df['datetime_object']]\n",
    "#val_df['year'] = [date.year for date in val_df['datetime_object']]\n",
    "test_df.dtypes\n",
    "#val_df = pd.read_csv('../input/train.csv', nrows = 10000000)\n",
    "#val_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e94990202009366436f78fdd66d546f40d4e85d"
   },
   "outputs": [],
   "source": [
    "# Reuse the above helper functions to add our features and generate the input matrix.\n",
    "#add_travel_vector_features(test_df)\n",
    "#test_X = get_input_matrix(test_df)\n",
    "#add_travel_vector_features(val_df)\n",
    "#val_df = val_df.dropna(how = 'any', axis = 'rows')\n",
    "#val_df = val_df[(val_df.abs_diff_longitude < 5.0) & (val_df.abs_diff_latitude < 5.0)]\n",
    "#val_X = get_input_matrix(val_df)\n",
    "# Predict fare_amount on the test set using our model (w) trained on the training set.\n",
    "#test_y_predictions = np.matmul(test_X, w).round(decimals = 2)\n",
    "#val_y_predictions = np.matmul(val_X, w).round(decimals = 2)\n",
    "#val_y = np.array(val_df['fare_amount'])\n",
    "\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#print(np.sqrt(mean_squared_error(val_y, val_y_predictions)))\n",
    "# Write the predictions to a CSV file which we can submit to the competition.\n",
    "#submission = pd.DataFrame(\n",
    "#    {'key': test_df.key, 'fare_amount': test_y_predictions},\n",
    "#    columns = ['key', 'fare_amount'])\n",
    "#submission.to_csv('submission.csv', index = False)\n",
    "\n",
    "#print(os.listdir('.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
